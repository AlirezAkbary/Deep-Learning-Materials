# Deep-Learning-Materials
These are the materials I designed for the [CE-40719 Fall 2020](http://ce.sharif.edu/courses/99-00/1/ce719-1/index.php/section/syllabus/file/syllabus) Deep Learning Course. 

# Quizzes
I have the responsibility to design the quizzes of this course in Farsi, assessing the general process of students' learning throughout the course. I try to diligently prepare them in order to make them completely instructive. Besides, I sometimes try to incorporate some ideas from diverse papers and design them step-by-step whereby students come up with new concepts and get familiar with them by themselves.

## Quiz 1
Covered topics:
+ Machine Learning Review
+ Universal approximation of MLP.

## Quiz 2
Covered topics:
+ Backward propagation in ResNet. Based on ICML 2016 tutorial **Deep Residual Networks, Deep Learning Gets Way Deeper** by Kaiming He.[(link)](https://icml.cc/2016/tutorials/icml2016_tutorial_deep_residual_networks_kaiminghe.pdf)
+ Optimization in neural networks, especially based on **Adam optimizer.** [(arxiv)](https://arxiv.org/abs/1412.6980)

## Quiz 3
Covered topics:
+ Convolutional neural networks important characteristics such as sparse connectivity and parameter sharing. Based on **Deep Learning Book**. [(link)](https://www.deeplearningbook.org/)
+ Introducing invariances in convolutional neural networks. Based on **Locally Scale-Invariant Convolutional Neural Network** by Kanazawa et al. [(arxiv)](https://arxiv.org/pdf/1412.5104) and **Deep Learning Book** [(link)](https://www.deeplearningbook.org/).

## Quiz 4
Covered topics:
+ Recurrent neural networks advantages to learn sequential data over architectures like MLP.
+ Transformers and attentions mechanisms advantages over RNNs and LSTMs. Based on **Attention is all you need** by Vaswani et al. [(arxiv)](https://arxiv.org/abs/1706.03762)

## Quiz 5
Covered topics:
+ Word2Vec (Skip-Gram) Embeddings and Skip-gram vs. Autoencoder.
+ GloVe: Global Vectors for Word Representation
+ Transformer Masked Attention. Based on **Attention is all you need** by Vaswani et al. [(arxiv)](https://arxiv.org/abs/1706.03762)

## Quiz 6
Covered topics:
+ Word2Vec (Skip-Gram) Embeddings and Skip-gram vs. Autoencoder.
+ GloVe: Global Vectors for Word Representation
+ Transformer Masked Attention. Based on **Attention is all you need** by Vaswani et al. [(arxiv)](https://arxiv.org/abs/1706.03762)